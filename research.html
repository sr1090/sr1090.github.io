<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, intial-scale=1.0">
	<meta https-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="stylesheet" type="text/css" href="style.css">
	<title>Research | Siddharth Rupavatharam</title>
</head>

<body>
    
    <div class="topleft" ><a href="index.html"><img src="images/noun_switch.svg" width='80px' height='60px'></a> </div>

	<div class="header">
    	    <h2> Research </h2>
    </div>

		<div class="container_projects">
			<div class="projects-desc-image-left"> 
				<img src="images/jawClenching.png" alt="EyeLight" width="300">
			</div>
			
			<div class="projects-desc-desc-right"> 
				 <strong> Towards In-Ear Inertial Jaw Clenching Detection:</strong>
					<ul>
						<li> Bruxism is a jaw-muscle condition characterized by repetitive clenching or grinding of teeth </li>
						<li> Existing methods of detecting jaw clenching towards diagnosing bruxism are either invasive or not very reliable </li>
						<li> As a first step towards building a reliable, non-invasive and light weight bruxism detector, we propose an eSense based in-ear inertial jaw clenching detection technique that detects peaks/dips in gyroscope vector magnitude </li>
						<li> We also present results from preliminary experiments that show an equal error rate of 1% when the person is stationary and 4% when moving </li><br>
					</ul>
			</div>
		<!-- </div> -->
		
		<!-- <div class="container_projects"> -->
			
			<div class="projects-desc-image-left">
				<iframe width="300" height="200" src="https://www.youtube.com/embed/Mno7Q2F0bNg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			</div>
		
			<div class="projects-desc-desc-right">
				<strong> HandSense</strong>
				<ul>
					<li> Head-mounted devices (HMD) for Augmented Reality (AR) are gaining traction thanks to a growing number of applications in the areas of image guided therapy, computer aided design, cargo packing, manufacturing and digital field service </li>
					<li> However, providing an always available, intuitive and user friendly input for these devices remains a challenging problem </li>
					<li> This paper explores recognizing dynamic, micro finger gestures using capacitive coupling for interacting with a head-mounted device. Electrodes are attached to fingertips of users gloves and capacitive coupling among all pairs of electrodes is measured quickly to infer the real-time spatial relationship between fingers </li>
					<li> The system is able to recognize fine, low-effort finger gestures, such as swiping, sliding, tap, double-tap. We evaluated our prototype with 14 gestures executed by 10 subjects and found a 97% accuracy of gesture recognition </li><br>
				 </ul>
			</div>
		<!-- </div> -->

		<!-- <div class="container_projects"> -->

			<div class="projects-desc-image-left">
				<img src="images/eyelight.png" alt="EyeLight" width="300" vertical-align="middle">
			</div>
			
			<div class="projects-desc-desc-right">
				<strong>EyeLight: Activity Sensing using Ceiling Photosensors</strong>
				<ul>
					<li> This project explores the feasibility of localizing and detecting activities of building occupants using visible light sensing across a mesh of light bulbs. Existing Visible Light activity sensing (VLS) techniques require either light sensors to be deployed on the floor or a person to carry a device </li>
					<li>  Our approach integrates photosensors with light bulbs and exploits the light reflected off the floor to achieve an entirely device-free and light source based system </li>
					<li> This forms a mesh of virtual light barriers across networked lights to track shadows cast by occupants. The design employs a synchronization circuit that implements a time division signaling scheme to differentiate between light sources and a sensitive sensing circuit to detect small changes in weak reflections. Sensor readings are fed into indoor supervised tracking algorithms as well as occupancy and activity recognition classifiers</li>
					<li>  Our prototype uses modified off-the-shelf LED light bulbs and is installed in a typical office conference room. We evaluate the performance of our system in terms of localization, occupancy estimation and activity classification, and find a 0.89m median localization error as well as 93.7% and 93.78% occupancy and activity classification accuracy, respectively </li>
				</ul>
			</div>
		</div>

</body>
</html>